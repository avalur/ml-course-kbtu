{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><img src=\"./images/logo_kbtu.png\" width=300 style=\"display: inline-block;\"></center> \n",
    "\n",
    "### Week 9, RNN practice\n",
    "\n",
    "<br />\n",
    "<br />\n",
    "November 5, 2022"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Source: https://github.com/andriygav/MachineLearningSeminars/tree/master/sem15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s3qWY0M5LA6r"
   },
   "source": [
    "### Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "K2_VhyWeteMB"
   },
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.image import imread\n",
    "from mpl_toolkits import mplot3d\n",
    "from matplotlib import gridspec\n",
    "from PIL import Image\n",
    "import io\n",
    "from urllib.request import urlopen\n",
    "from lime import lime_image\n",
    "from skimage.segmentation import mark_boundaries\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "import numpy as np\n",
    "import requests\n",
    "import torch\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "UCn8xDPhteMB"
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h61Q8xrnqDUV"
   },
   "source": [
    "### Set the code execution device (cpu/cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mkFMQL4RqDUV",
    "outputId": "1abaa6d8-02d0-4450-8e7e-0633d9b61394"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8rh0dr4tqDUc"
   },
   "source": [
    "### Recurrent neural network (seq2seq architecture, encoder-decoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"./images/enc_dec_linear_out-min.png\" width=600></center> \n",
    "\n",
    "-----\n",
    "Source: https://lena-voita.github.io/nlp_course/seq2seq_and_attention.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ia0PcGE__GBh"
   },
   "source": [
    "#### Useful code for model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "dbVvmM0mqDUe"
   },
   "outputs": [],
   "source": [
    "def batch_generator(dataset, char2idx, batch_size=64, shuffle=True):\n",
    "    X, Y = dataset[:-1], dataset[1:]\n",
    "    \n",
    "    PAD = char2idx['<PAD>']\n",
    "    n_samples = len(X)\n",
    "\n",
    "    # list of indexes generation\n",
    "    list_of_indexes = np.linspace(\n",
    "        0, n_samples - 1, n_samples, dtype=np.int64)\n",
    "    List_X = []\n",
    "    List_Y = []\n",
    "    \n",
    "    if shuffle:\n",
    "        np.random.shuffle(list_of_indexes)        \n",
    "\n",
    "    # generate a list of indices, by these indices,\n",
    "    # make a new shuffled list of tokens and tags\n",
    "    for indx in list_of_indexes:\n",
    "            List_X.append(X[indx])\n",
    "            List_Y.append(Y[indx])\n",
    "    \n",
    "    n_batches = n_samples // batch_size\n",
    "    if n_samples % batch_size != 0:\n",
    "        n_batches += 1\n",
    "        \n",
    "    # For each k yield pair of x and y\n",
    "    for k in range(n_batches):\n",
    "        this_batch_size = batch_size\n",
    "    \n",
    "        # if we have the last batch, then it needs to be cut\n",
    "        if k == n_batches - 1:\n",
    "            if n_samples % batch_size > 0:\n",
    "                this_batch_size = n_samples % batch_size\n",
    "                \n",
    "        This_X = List_X[k*batch_size:k*batch_size + this_batch_size]\n",
    "        This_Y = List_Y[k*batch_size:k*batch_size + this_batch_size]\n",
    "        \n",
    "        This_X_line = [\n",
    "                       [char2idx.get(char, 0) for char in sent]\\\n",
    "                       for sent in This_X]\n",
    "        This_Y_line = [\n",
    "                       [char2idx.get('<START>', 0)]\\\n",
    "                       + [char2idx.get(char, 0) for char in sent]\\\n",
    "                       + [char2idx.get('<FINISH>', 0)]\\\n",
    "                       for sent in This_Y]\n",
    "\n",
    "        List_of_length_x = [len(sent) for sent in This_X_line]\n",
    "        length_of_sentence_x = max(List_of_length_x)\n",
    "        List_of_length_y = [len(sent) for sent in This_Y_line]\n",
    "        length_of_sentence_y = max(List_of_length_y)\n",
    "\n",
    "        x_arr = np.ones(shape=[this_batch_size, length_of_sentence_x])*PAD\n",
    "        y_arr = np.ones(shape=[this_batch_size, length_of_sentence_y])*PAD\n",
    "\n",
    "        for i in range(this_batch_size):\n",
    "            x_arr[i, :len(This_X_line[i])] = This_X_line[i]\n",
    "            y_arr[i, :len(This_Y_line[i])] = This_Y_line[i]\n",
    "\n",
    "        x = torch.LongTensor(x_arr)\n",
    "        y = torch.LongTensor(y_arr)\n",
    "        lengths = torch.LongTensor(List_of_length_x)\n",
    "\n",
    "        yield x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "yJbyhRfzqDUe"
   },
   "outputs": [],
   "source": [
    "def train_on_batch(model, batch_of_x, batch_of_y, optimizer, loss_function):\n",
    "    encoder, decoder = model\n",
    "    encoder.train()\n",
    "    decoder.train()\n",
    "    encoder.zero_grad()\n",
    "    decoder.zero_grad()\n",
    "    \n",
    "    d, h, c = encoder(batch_of_x.to(encoder.device))\n",
    "    output = decoder(\n",
    "        batch_of_y.to(decoder.device), \n",
    "        h=h.to(decoder.device)[:, -decoder.num_layers:, :], \n",
    "        c=c.to(decoder.device)[:, -decoder.num_layers:, :])\n",
    "\n",
    "    loss = loss_function(output[:, :-1, :].transpose(1, 2), batch_of_y.to(decoder.device)[:, 1:])\n",
    "    \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    return loss.cpu().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "fdci60aaqDUe"
   },
   "outputs": [],
   "source": [
    "def train_epoch(train_generator, model, loss_function, optimizer):\n",
    "    epoch_loss = 0\n",
    "    total = 0\n",
    "    for it, (batch_of_x, batch_of_y) in enumerate(train_generator):\n",
    "        local_loss = train_on_batch(\n",
    "            model, batch_of_x, batch_of_y, optimizer, loss_function)\n",
    "        train_generator.set_postfix({'train batch loss': local_loss})\n",
    "\n",
    "        epoch_loss += local_loss * len(batch_of_x)\n",
    "        total += len(batch_of_x)\n",
    "    \n",
    "    return epoch_loss / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "1k43LkCVqDUf"
   },
   "outputs": [],
   "source": [
    "def trainer(count_of_epoch, \n",
    "            batch_size,\n",
    "            model,\n",
    "            dataset,\n",
    "            char2idx,\n",
    "            loss_function,\n",
    "            optimizer,):\n",
    "    iterations = tqdm(range(count_of_epoch))\n",
    "\n",
    "    for it in iterations:\n",
    "        optima = optimizer\n",
    "\n",
    "        number_of_batch = len(dataset) // batch_size + (len(dataset) % batch_size > 0)\n",
    "        generator = tqdm(\n",
    "            batch_generator(dataset, char2idx, batch_size), \n",
    "            leave=False, total=number_of_batch)\n",
    "        \n",
    "        epoch_loss = train_epoch(\n",
    "            train_generator = generator, model = model, \n",
    "            loss_function = loss_function, \n",
    "            optimizer = optima)\n",
    "\n",
    "        iterations.set_postfix({'train epoch loss': epoch_loss})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f4gkPhl-_GBj"
   },
   "source": [
    "### Nueral network model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "cHykQdETqDUd"
   },
   "outputs": [],
   "source": [
    "class Encoder(torch.nn.Module):\n",
    "    @property\n",
    "    def device(self):\n",
    "        return next(self.parameters()).device\n",
    "        \n",
    "    def __init__(self,\n",
    "                 vocab_dim,\n",
    "                 emb_dim = 10, \n",
    "                 hidden_dim = 10,\n",
    "                 num_layers = 3,\n",
    "                 bidirectional = False):\n",
    "        super(Encoder, self).__init__()\n",
    "        \n",
    "        self.num_direction = int(bidirectional + 1)\n",
    "        self.emb_dim = emb_dim # learnable embedding matrix\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        self.embedding = torch.nn.Embedding(vocab_dim, emb_dim)\n",
    "\n",
    "        self.encoder = torch.nn.LSTM(\n",
    "            emb_dim, hidden_dim, num_layers, bidirectional=bidirectional)\n",
    "        \n",
    "    def forward(self, input):\n",
    "        input = self.embedding(input) # shape (batch_size, seq_len, emb_dim)\n",
    "        input = torch.transpose(input, 0, 1) # shape (seq_len, batch_size, emb_dim)\n",
    "        d, (h, c) = self.encoder(input)\n",
    "        return d, torch.transpose(h, 0, 1), torch.transpose(c, 0, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "9ZlLDdTUqDUd"
   },
   "outputs": [],
   "source": [
    "class Decoder(torch.nn.Module):\n",
    "    @property\n",
    "    def device(self):\n",
    "        return next(self.parameters()).device\n",
    "\n",
    "    def __init__(self,\n",
    "                 vocab_dim,\n",
    "                 output_dim,\n",
    "                 emb_dim = 10, \n",
    "                 hidden_dim = 10,\n",
    "                 num_layers = 3,\n",
    "                 bidirectional = False):\n",
    "        super(Decoder, self).__init__()\n",
    "        \n",
    "        self.num_direction = int(bidirectional + 1)\n",
    "        self.emb_dim = emb_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.embedding = torch.nn.Embedding(vocab_dim, self.emb_dim)\n",
    "\n",
    "        self.decoder = torch.nn.LSTM(\n",
    "            emb_dim, hidden_dim, num_layers, bidirectional = bidirectional)\n",
    "\n",
    "        self.linear = torch.nn.Linear(\n",
    "            self.num_direction*hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, real=None, h=None, c=None, max_len=50):\n",
    "        batch_size = 1\n",
    "        if h is not None:\n",
    "            batch_size = h.shape[0]\n",
    "        if c is not None:\n",
    "            batch_size = c.shape[0]\n",
    "        if real is not None:\n",
    "            batch_size = real.shape[0]\n",
    "\n",
    "\n",
    "        if real is not None:\n",
    "            input = self.embedding(real)\n",
    "\n",
    "            if h is None:\n",
    "                h = torch.randn(\n",
    "                    (batch_size, self.num_layers, self.num_direction*self.hidden_dim)).to(\n",
    "                        self.device\n",
    "                    )\n",
    "            if c is None:\n",
    "                c = torch.randn(\n",
    "                    (batch_size, self.num_layers, self.num_direction*self.hidden_dim)).to(\n",
    "                        self.device\n",
    "                    )\n",
    "\n",
    "            input = torch.transpose(input, 0, 1)\n",
    "            h = torch.transpose(h, 0, 1)\n",
    "            c = torch.transpose(c, 0, 1)\n",
    "            d, _ = self.decoder(input, (h, c))\n",
    "            answers = self.linear(d)\n",
    "        else:\n",
    "            input = self.embedding(\n",
    "                torch.tensor(\n",
    "                    [[char2idx['<START>']] for _ in range(\n",
    "                        batch_size)]).long().to(\n",
    "                        self.device\n",
    "                    )\n",
    "                )\n",
    "\n",
    "            if h is None:\n",
    "                h = torch.randn(\n",
    "                    (batch_size, self.num_layers, self.num_direction*self.hidden_dim)).to(\n",
    "                        self.device\n",
    "                    )\n",
    "            if c is None:\n",
    "                c = torch.randn(\n",
    "                    (batch_size, self.num_layers, self.num_direction*self.hidden_dim)).to(\n",
    "                        self.device\n",
    "                    )\n",
    "\n",
    "            input = torch.transpose(input, 0, 1)\n",
    "            h = torch.transpose(h, 0, 1)\n",
    "            c = torch.transpose(c, 0, 1)\n",
    "\n",
    "            answers = torch.zeros(\n",
    "                (max_len, input.shape[1], self.output_dim)).to(\n",
    "                    self.device)\n",
    "                \n",
    "            for i in range(max_len):\n",
    "                d, (h, c) = self.decoder(input, (h, c))\n",
    "                answers[i, :, :] = self.linear(d)[0]\n",
    "                input = self.embedding(\n",
    "                    torch.argmax(answers[i:i+1, :, :], dim=-1))\n",
    "\n",
    "        return torch.transpose(answers, 0, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c7KY9OTT_GBm"
   },
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "vAM9RmjAqDUf"
   },
   "outputs": [],
   "source": [
    "text = open('input.txt', 'r', encoding='utf-8').read() # should be simple plain text file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "UtgW1ExSqDUf"
   },
   "outputs": [],
   "source": [
    "char2idx = {'<PAD>':0, '<UNK>': 1, '<START>': 2, '<FINISH>': 3}\n",
    "idx2char = {0: '<PAD>', 1: '<UNK>', 2: '<START>', 3: '<FINISH>'}\n",
    "for item in list(set(text)):\n",
    "    char2idx[item] = len(char2idx)\n",
    "    idx2char[char2idx[item]] = item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8J_D3vo8qDUf",
    "outputId": "eb391a3e-8791-4404-cc02-e63f27dcf958"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3758"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = [sent.strip() for sent in text.split('\\n') if len(sent.strip()) > 20 and \n",
    "           len(sent.strip()) < 300 ]\n",
    "len(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZPZ5LTRS_GBo"
   },
   "source": [
    "### Model initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "ePn7fVdCqDUf"
   },
   "outputs": [],
   "source": [
    "encoder = Encoder(vocab_dim=len(char2idx), \n",
    "                  num_layers=2, emb_dim=100, hidden_dim=100)\n",
    "encoder.to(device)\n",
    "decoder = Decoder(vocab_dim=len(char2idx), \n",
    "                  output_dim=len(char2idx), num_layers=2, emb_dim=100, hidden_dim=100)\n",
    "decoder.to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(\n",
    "    list(encoder.parameters()) + list(decoder.parameters()), lr=1e-3)\n",
    "loss_function = torch.nn.CrossEntropyLoss(ignore_index=char2idx['<PAD>'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EgZb6LOH_GBp"
   },
   "source": [
    "### Model quality before training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2A7wODAGq-lQ",
    "outputId": "9f9bb723-2e62-41f1-c16b-00650467050b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "дввввдвдввдв<UNK><UNK>в<UNK>в<UNK>в<UNK>в<UNK>в<UNK>в<UNK>в<UNK>в<UNK>в<UNK>в<UNK>в<UNK>в<UNK>в<UNK>в<UNK>в<UNK>в<UNK>в<UNK>в<UNK>в<UNK>в<UNK>в<UNK>в<UNK>в<UNK>в<UNK>в<UNK>в<UNK>в<UNK>в<UNK>в<UNK>в<UNK>в<UNK>в<UNK>в<UNK>в<UNK>в<UNK>в<UNK>в<UNK>в<UNK>в<UNK>в<UNK>в<UNK>в<UNK>в<UNK>\n",
      "ЗЗЗЗЗЗвФФOOOьФФOOGФФOOGввФФOOOёФOOьФФOOGФФOOGввФФOOOёФOOьФФOOGФФOOGввФФOOOёФOOьФФOOGФФOOGввФФOOOёФOO\n",
      "ФФФФOOGввФФOOGввФФOOOёФOOьФФOOGФФOOGввФФOOOёФOOьФФOOGФФOOGввФФOOOёФOOьФФOOGФФOOGввФФOOOёФOOьФФOOGФФO\n",
      "VёёььььФФOOьФФOOGФФOOGввФФOOOёФOOьФФOOGФФOOGввФФOOOёФOOьФФOOGФФOOGввФФOOOёФOOьФФOOGФФOOGввФФOOOёФOOь\n",
      "ццццGвФФФOOOьФФOOGФФOOGввФФOOOёФOOьФФOOGФФOOGввФФOOOёФOOьФФOOGФФOOGввФФOOOёФOOьФФOOGФФOOGввФФOOOёФOO\n",
      "eФФOOьФOФOGФOФOOGввФФOOGввФФOOOёФOOьФФOOGФФOOGввФФOOOёФOOьФФOOGФФOOGввФФOOOёФOOьФФOOGФФOOGввФФOOOёФO\n",
      "Oдддвдввдвyдввyдвдввyyддввдвyyдвдввyyддввдвyyдвдввyyддввдвyyдвдввyyддввдвyyдвдввyyддввдвyyдвдввyyддв\n",
      "CддввФФOOOьФФOOGФФOOGввФФOOOёФOOьФФOOGФФOOGввФФOOOёФOOьФФOOGФФOOGввФФOOOёФOOьФФOOGФФOOGввФФOOOёФOOьФ\n",
      "вввв<UNK><UNK>в<UNK>в<UNK>в<UNK>в<UNK>в<UNK>в<UNK>в<UNK>в<UNK>в<UNK>в<UNK>в<UNK>в<UNK>в<UNK>в<UNK>в<UNK>в<UNK>в<UNK>в<UNK>в<UNK>в<UNK>в<UNK>в<UNK>в<UNK>в<UNK>в<UNK>в<UNK>в<UNK>в<UNK>в<UNK>в<UNK>в<UNK>в<UNK>в<UNK>в<UNK>в<UNK>в<UNK>в<UNK>в<UNK>в<UNK>в<UNK>в<UNK>в<UNK>в<UNK>в<UNK>в<UNK>в<UNK>в<UNK>\n",
      "<PAD>ъъвввФOOёФOOьФФOOGФФOOGввФФOOOёФOOьФФOOGФФOOGввФФOOOёФOOьФФOOGФФOOGввФФOOOёФOOьФФOOGФФOOGввФФOOOёФO\n"
     ]
    }
   ],
   "source": [
    "for _ in range(10):\n",
    "    indexes = torch.argmax(\n",
    "        decoder(max_len=100,\n",
    "                h=0.1*torch.randn(\n",
    "                    (1, decoder.num_layers, decoder.num_direction*decoder.hidden_dim)).to(\n",
    "                        decoder.device\n",
    "                ), \n",
    "                c=torch.randn(\n",
    "                    (1, decoder.num_layers, decoder.num_direction*decoder.hidden_dim)).to(\n",
    "                    decoder.device\n",
    "                )), dim=-1).detach().cpu().numpy()[0]\n",
    "    list_of_char = []\n",
    "    for idx in indexes:\n",
    "        if idx == char2idx['<FINISH>']:\n",
    "            break\n",
    "        list_of_char.append(idx2char[idx])\n",
    "    print(''.join(list_of_char))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2kFxWjXE_GBq"
   },
   "source": [
    "### Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "IIH2giImqDUg"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5d68dd167834b71bd8edbfb6685cf2b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/59 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/59 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/59 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/59 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/59 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/59 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/59 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/59 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/59 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/59 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer(count_of_epoch=10,\n",
    "        batch_size=64,\n",
    "        model=(encoder, decoder),\n",
    "        dataset=dataset, \n",
    "        char2idx=char2idx,\n",
    "        loss_function=loss_function,\n",
    "        optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lhV_ulJ4_GBq"
   },
   "source": [
    "### Model quality after training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ed8nkY5dqDUg",
    "outputId": "e3ec7055-8e5c-435a-97bc-a6a43335a251"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ютрить в проворил в проворил в проворил в проворил в проворил в проворил в проворил в проворил в про\n",
      "ря в проворил в проворил в проворил в проворил в проворил в проворил в проворил в проворил в провори\n",
      "ят в проворил в проворил в проворил в проворил в проворил в проворил в проворил в проворил в провори\n",
      "дднне не подоворил в проворил в проворил в проворил в проворил в проворил в проворил в проворил в пр\n",
      "вде не проворил в проворил в проворил в проворил в проворил в проворил в проворил в проворил в прово\n",
      "лран в проворил в проворил в проворил в проворил в проворил в проворил в проворил в проворил в прово\n",
      "– не не подоворил в проворил в проворил в проворил в проворил в проворил в проворил в проворил в про\n",
      "lr провил в проворил в проворил в проворил в проворил в проворил в проворил в проворил в проворил в \n",
      "– не проворил в проворил в проворил в проворил в проворил в проворил в проворил в проворил в провори\n",
      "и не проворил в проворил в проворил в проворил в проворил в проворил в проворил в проворил в провори\n"
     ]
    }
   ],
   "source": [
    "for _ in range(10):\n",
    "    indexes = torch.argmax(\n",
    "        decoder(max_len=100,\n",
    "                h=0.1*torch.randn(\n",
    "                    (1, decoder.num_layers, decoder.num_direction*decoder.hidden_dim)).to(\n",
    "                        decoder.device\n",
    "                ), \n",
    "                c=torch.randn(\n",
    "                    (1, decoder.num_layers, decoder.num_direction*decoder.hidden_dim)).to(\n",
    "                    decoder.device\n",
    "                )), dim=-1).detach().cpu().numpy()[0]\n",
    "    list_of_char = []\n",
    "    for idx in indexes:\n",
    "        if idx == char2idx['<FINISH>']:\n",
    "            break\n",
    "        list_of_char.append(idx2char[idx])\n",
    "    print(''.join(list_of_char))"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "rise": {
   "transition": "none"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
