{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div align=\"center\">\n",
    "    <img src=\"images/logo_kbtu.png\" alt=\"logo_kbtu\" />\n",
    "</div>\n",
    "\n",
    "### RNN demo\n",
    "\n",
    "<br />\n",
    "<br />\n",
    "November 5, 2022"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data has 1821625 characters, 146 unique.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Minimal character-level Vanilla RNN model. Written by Andrej Karpathy (@karpathy)\n",
    "BSD License\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "\n",
    "# data I/O\n",
    "data = open('input.txt', 'r', encoding='utf-8').read() # should be simple plain text file\n",
    "chars = list(set(data))\n",
    "data_size, vocab_size = len(data), len(chars)\n",
    "print('data has %d characters, %d unique.' % (data_size, vocab_size))\n",
    "char_to_ix = { ch:i for i,ch in enumerate(chars) }\n",
    "ix_to_char = { i:ch for i,ch in enumerate(chars) }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Annotation\n",
      "\n",
      "Самый сложный, самый многоуровневый и неоднозначный из романов Достоевского, который критики считали то «интеллектуальным детективом», то «ранним постмодернизмом», то – «лучшим из произведений о загадочной русской душе». Роман, легший в основу десятков экранизаций – от предельно точных до самых отвлеченных, – но не утративший своей духовной силы…\n",
      "\n",
      "\n",
      "\n",
      "* * *\n",
      "\n",
      "Посвящается Анне Григорьевне Достоевской\n",
      "\n",
      "\n",
      "Истинно, истинно говорю вам: если пшеничное зерно, падши в землю, не умрет, то останется одно; а если умрет, то принесет много плода.\n",
      "\n",
      "(Евангелие от Иоанна, гл. XII, ст. 24).\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# I don’t quite understand why in literature lessons schoolchildren are offered to read Dostoevsky’s Crime and Punishment, \n",
    "# and not this novel..\n",
    "print(data[:590])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ê ’ К é m X Ч ” Я р Г F и 0 P т M с e Ц ( G 9 ь г Э x q 2 g S N n A f î „ — У p Щ ] , ю o i в z ? « Б d Ф 1 – I - Q п ё П к à R х л э Ш В з й : Т » V Ж u c ъ 8 у )   s h l Д 6 D И k м t а ч è Ç М ж ! ; T C Р Х Н y … [ O J “ r b А щ 7 v С н a ‑ Л О ц Е З \\n ф е L ш j 5 . Ю U д я B 4 о * 3 ы б'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join(chars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "hidden_size = 100 # size of hidden layer of neurons\n",
    "seq_length = 25 # number of steps to unroll the RNN for\n",
    "learning_rate = 1e-1\n",
    "\n",
    "# model parameters\n",
    "Wxh = np.random.randn(hidden_size, vocab_size)*0.01 # input to hidden\n",
    "Whh = np.random.randn(hidden_size, hidden_size)*0.01 # hidden to hidden\n",
    "Why = np.random.randn(vocab_size, hidden_size)*0.01 # hidden to output\n",
    "bh = np.zeros((hidden_size, 1)) # hidden bias\n",
    "by = np.zeros((vocab_size, 1)) # output bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<div align=\"left\">\n",
    "    <img src=\"images/rnn_char_level_example.jpg\" width=400/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Loss function\n",
    "\n",
    " * forward pass for loss calculation\n",
    " * backpropagation of error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Let us denote the components of the probability vector of symbols (classes) at the RNN output as\n",
    "$$ p_k = \\frac{e^{f_k}}{\\sum\\limits_j e^{f_j}}$$\n",
    "\n",
    "Then the value of the loss function on the next object $x_i$\n",
    "$$ L_i = -\\ln (p_{y_i})$$\n",
    "\n",
    "Then, by direct calculation of the derivative, we can obtain\n",
    "$$ \\frac{\\partial L_i}{\\partial f_k} = p_k - 1(y_i = k)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "def lossFun(inputs, targets, hprev):\n",
    "    \"\"\"\n",
    "    inputs, targets are both list of integers\n",
    "    hprev is Hx1 array of initial hidden state\n",
    "    returns the loss, gradients on model parameters, and last hidden state\n",
    "    \"\"\"\n",
    "    xs, hs, ys, ps = {}, {}, {}, {}\n",
    "    hs[-1] = np.copy(hprev)\n",
    "    loss = 0\n",
    "    \n",
    "    # forward pass\n",
    "    for t in range(len(inputs)):\n",
    "        xs[t] = np.zeros((vocab_size, 1)) # encode in 1-of-k representation\n",
    "        xs[t][inputs[t]] = 1\n",
    "        hs[t] = np.tanh(np.dot(Wxh, xs[t]) + np.dot(Whh, hs[t-1]) + bh) # hidden state\n",
    "        ys[t] = np.dot(Why, hs[t]) + by # unnormalized log probabilities for next chars\n",
    "        ps[t] = np.exp(ys[t]) / np.sum(np.exp(ys[t])) # probabilities for next chars\n",
    "        loss += -np.log(ps[t][targets[t],0]) # softmax (cross-entropy loss)\n",
    "    \n",
    "    # backward pass: compute gradients going backwards\n",
    "    dWxh, dWhh, dWhy = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why)\n",
    "    dbh, dby = np.zeros_like(bh), np.zeros_like(by)\n",
    "    dhnext = np.zeros_like(hs[0])\n",
    "    for t in reversed(range(len(inputs))):\n",
    "        dy = np.copy(ps[t])\n",
    "        dy[targets[t]] -= 1 # backprop into y. see http://cs231n.github.io/neural-networks-case-study/#grad if confused here\n",
    "        dWhy += np.dot(dy, hs[t].T)\n",
    "        dby += dy\n",
    "        dh = np.dot(Why.T, dy) + dhnext # backprop into h\n",
    "        dhraw = (1 - hs[t] * hs[t]) * dh # backprop through tanh nonlinearity\n",
    "        dbh += dhraw\n",
    "        dWxh += np.dot(dhraw, xs[t].T)\n",
    "        dWhh += np.dot(dhraw, hs[t-1].T)\n",
    "        dhnext = np.dot(Whh.T, dhraw)\n",
    "    for dparam in [dWxh, dWhh, dWhy, dbh, dby]:\n",
    "        np.clip(dparam, -5, 5, out=dparam) # clip to mitigate exploding gradients\n",
    "    return loss, dWxh, dWhh, dWhy, dbh, dby, hs[len(inputs)-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "def sample(h, seed_ix, n):\n",
    "    \"\"\" \n",
    "    sample a sequence of integers from the model \n",
    "    h is memory state, seed_ix is seed letter for first time step\n",
    "    \"\"\"\n",
    "    x = np.zeros((vocab_size, 1))\n",
    "    x[seed_ix] = 1\n",
    "    ixes = []\n",
    "    for t in range(n):\n",
    "        h = np.tanh(np.dot(Wxh, x) + np.dot(Whh, h) + bh)\n",
    "        y = np.dot(Why, h) + by\n",
    "        p = np.exp(y) / np.sum(np.exp(y))\n",
    "        ix = np.random.choice(range(vocab_size), p=p.ravel())\n",
    "        x = np.zeros((vocab_size, 1))\n",
    "        x[ix] = 1\n",
    "        ixes.append(ix)\n",
    "    return ixes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### The main loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "n, p = 0, 0\n",
    "mWxh, mWhh, mWhy = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why)\n",
    "mbh, mby = np.zeros_like(bh), np.zeros_like(by) # memory variables for Adagrad\n",
    "smooth_loss = -np.log(1.0/vocab_size)*seq_length # loss at iteration 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----\n",
      " …к8МиR4ЗаOо]«:îcнёG“Оm—sСЧQcD*бфdоîр5z7О!к»Б]ЛvФj1BÇAPЖпёбЦz7Е]ЯП.ИшупàСЧBу.ggzс’Б]3ПВ…ЗЯюЦiApRь-—2РмQeьЧФФо»Oà2вk1Лз,cV7èуyОЮл»xОиНЭèКNЩЕэжV Ш-‑kи;Я PSвu.e1Q ,чыS6пЛКзрG—МbЭИbДeяxш-ёщux8шфрБуbyи.Pця] \n",
      "----\n",
      "iter 0, loss: 124.590160\n",
      "----\n",
      " кенемясЯл Она теробыэтлороу, поком писсиври э сякнед ч в викозавожьшотнасжлетаде в, дот » ченедтадигбыЯере, в всвысибидсзклремниен, и. квим пейвьмеяскя кибоне нтете босвФу  ночстсстоки сндвредоло сяш. \n",
      "----\n",
      "iter 1000, loss: 93.683423\n",
      "----\n",
      " то, м. Сх, с– о по эь,о, Подех рым, м……сжозевей а накОксьме ох См? рнен нык ом как, х ом  зне нажери накакемям, прино омустьже го ны?кова\n",
      "ь Нопе Семнатьтед ТавоС. С лемнак ннень о..ь, Иох,. мана юютат \n",
      "----\n",
      "iter 2000, loss: 76.426299\n",
      "----\n",
      " еай и – на, по, ’валак, сог вожобы башы Но праа вер ч.. в? о ио, сатам ротоде дара ледавидоде др солтыша ипбо бысосретенушдидгсуло, аеро бdвив аголог; ны вибони себкарагредабисорег ра госе бnдогорарас \n",
      "----\n",
      "iter 3000, loss: 69.911899\n",
      "----\n",
      "  никда тоженьет. пренсаз, кажя, Дотослотатиузамельнаскать, днецедозот бе…ю…ливу Дду, проваледнавит е провеугосто веитвс!н метсе на стяльаз прашештаентаденою, я ствазал, но аравььлос!ипрокить за бышь,  \n",
      "----\n",
      "iter 4000, loss: 66.744771\n",
      "----\n",
      "  этию вся ка завивняе на на сты ваа нае ся ныю вать зе, Яокы, ся о с ве вы мымикацы ею прозата, «се тысь.\n",
      "\n",
      "– Lме сеся ов естойсВусь, каго я кнечь и, вМдесесьсе, выю я: – Иной.\n",
      "\n",
      "– Этоа сме,,\n",
      " выл.\n",
      "\n",
      "– – \n",
      "----\n",
      "iter 5000, loss: 63.997953\n",
      "----\n",
      " енни егчтосемо нарьтой о ливай, с пррей ком верстаят а себуюм в военей ней? Важя, меннелком на Чковернвей, вся, ссумецею кудоеваегого, всий невей сомо, дтвосехокье я на жей етсегодма, жевоешее бестики \n",
      "----\n",
      "iter 6000, loss: 61.864548\n",
      "----\n",
      " азну-толи, зимы тасуввот ит овие Пезже ными муже мосутаратсеиша вго. – нестать – – ко вси полою но говь в бодна нобыл казсовговим и заз. – ото бешь виглери сл…\n",
      "\n",
      "\n",
      "лацлез», «дови ной Фемя, бвретареама э \n",
      "----\n",
      "iter 7000, loss: 61.538207\n",
      "----\n",
      " е нану вошунем гулижко десям нол Пылаского следого гопоени. Оно, чеснай вошен тодиЗьраю слостыр заспоремундаржи не присс, бы но угой чероспровача бешилько отора ачал, кае, редлнерЗушел ся м оспроватли \n",
      "----\n",
      "iter 8000, loss: 60.801779\n",
      "----\n",
      " гох гризорго я крегой квя узатем, комол – лак, нем игсвадаегнлотиль, вимурсе еском лазсину редной слошденногело тоднакчамих тамуставоистиллостолукам фак: т тиннугнем, миша, в дер олтие порои и не «и о \n",
      "----\n",
      "iter 9000, loss: 60.370614\n",
      "----\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "сизавик, не нистем и ужемиЗ, нориилоичзит; –\n",
      "\n",
      "Теныбитос\n",
      "ещеле? Тем Титьсл гозведалая!\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "e, тредвуждцаше полешьх палки Тащела, дризал, втреледатьми?\n",
      "\n",
      "ъни\n",
      "\n",
      "\n",
      "кик ниль мили, непеиквигбужня \n",
      "----\n",
      "iter 10000, loss: 60.344238\n"
     ]
    }
   ],
   "source": [
    "while n < 10**4 + 1000: #True \n",
    "    # prepare inputs (we're sweeping from left to right in steps seq_length long)\n",
    "    if p+seq_length+1 >= len(data) or n == 0: \n",
    "        hprev = np.zeros((hidden_size,1)) # reset RNN memory\n",
    "        p = 0 # go from start of data\n",
    "    inputs = [char_to_ix[ch] for ch in data[p:p+seq_length]]\n",
    "    targets = [char_to_ix[ch] for ch in data[p+1:p+seq_length+1]]\n",
    "  \n",
    "    # sample from the model now and then\n",
    "    if n % 1000 == 0:\n",
    "        sample_ix = sample(hprev, inputs[0], 200)\n",
    "        txt = ''.join(ix_to_char[ix] for ix in sample_ix)\n",
    "        print('----\\n %s \\n----' % (txt, ))\n",
    "  \n",
    "    # forward seq_length characters through the net and fetch gradient\n",
    "    loss, dWxh, dWhh, dWhy, dbh, dby, hprev = lossFun(inputs, targets, hprev)\n",
    "    smooth_loss = smooth_loss * 0.999 + loss * 0.001\n",
    "    if n % 1000 == 0: \n",
    "        print('iter %d, loss: %f' % (n, smooth_loss)) # print progress\n",
    "    \n",
    "    # perform parameter update with Adagrad\n",
    "    for param, dparam, mem in zip([Wxh, Whh, Why, bh, by], \n",
    "                                  [dWxh, dWhh, dWhy, dbh, dby], \n",
    "                                  [mWxh, mWhh, mWhy, mbh, mby]):\n",
    "        mem += dparam * dparam\n",
    "        param += -learning_rate * dparam / np.sqrt(mem + 1e-8) # adagrad update\n",
    "  \n",
    "    p += seq_length # move data pointer\n",
    "    n += 1 # iteration counter "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Till $10^4$ iterations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "----\n",
    "\n",
    " шîрhUНч-rGexêШнAфРЮuГB“аёЛУz[нОp,tцgйеБ2Ç0q(ЕКжКQêGnlИмxqЩGЕЦ[:qы ЮkщiжxдЭ‑IgGёuД2f–щсйТашèиЕfIФбàXЭтЯРЧКkХЧVjтзЦгyЮчp…1шБ”QgХîÇydиb*”иЧ’,TX.,рuынBUmксхêШ(–XВяGАЭ,M…азNЮ„;SсДбВ\n",
    "ШДЦ)сg*1n4OСмlБTхT»»Лhè \n",
    "\n",
    "----\n",
    "\n",
    "#### iter 0, loss: 124.590170\n",
    "\n",
    "\n",
    "----\n",
    "\n",
    "  это бым вый, из о четоть вына скицса что. Ие стамыу, свложекьмисти сти на риз алум сщитикая темвы, чаль и, к к вычыо ивсе мазая, бывак, у мавеет вымю сол, ельсты с вышенум свотеже былилю казелеу выта \n",
    "\n",
    "----\n",
    "\n",
    "\n",
    "#### iter 5000, loss: 62.775185\n",
    "\n",
    "----\n",
    "\n",
    "Подарал прел гикруИ нексЦа оторуга и тоязарнийам нчегага: пуда, погоь, и равка ниголю и семи, отвы Имарачеля, и балая —стотвя не до жегдровитя и ста даяхоль! ят гдескнее  неюл\n",
    "\n",
    "– Веенить идлю пробыль \n",
    "\n",
    "----\n",
    "\n",
    "#### iter 10000, loss: 59.253950\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Till $10^5$ iterations\n",
    "\n",
    "----\n",
    "\n",
    "  продут – обаищести, дебы».\n",
    "\n",
    "– Тытьщиныма катеры было вашила сви тутя? Возотерия, чута? Повзал его, и впрому с выродился, что голо редел, лоб, верю. В леть еще поребяка, и слод, хонул в деть, вупушул, \n",
    "\n",
    "----\n",
    "\n",
    "**iter 100000, loss: 49.823363**\n",
    "\n",
    "----\n",
    "\n",
    "### $5*10^5$ iterations\n",
    "\n",
    "----\n",
    "\n",
    "крадание зазаженгий тою отвелось фастие, идо, кождо, что чем пришкоже, я камо кара сих-с теже есем патин иро-нать помне не начанно полили», боло. Если, это-то и дравствы» и молод не дурнать не прогору \n",
    "\n",
    "----\n",
    "\n",
    "**iter 500000, loss: 46.207874**\n",
    " "
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
