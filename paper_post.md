# Describe a modern paper or your machine learning project

The area is developing very quickly, so you constantly have to follow or "touch" the latest achievements — let's practice doing it!

Almost all papers in the field of Machine Learning are freely available on the Internet, usually at [arxiv.org](http://arxiv.org).

## How do I find an paper worth reading?

Lots of options!

- Reach links to scientific papers from news about the latest achievements of AI, which are [very] (https://neurohive.io/en/applications/control-flag-intel-automatic-code-debugging-tool/) and [very] (https://www.rbth.com/science-and-tech/334195-yandex-introduces-automatic-english-russian-video-dubbing-feature) [many](https://deepmind.com/blog/paper/AlphaFold-Using-AI-for-scientific-discovery)
- Read in detail one of the papers mentioned in the lectures of the course (see links in the slides)
- See what is the latest achievement (State of the Art or SotA) in a particular problem. Good resource tracking SotA in a large class of problems - [https://paperswithcode.com](https://paperswithcode.com/) . As an example, you can look at:
  - [Mask attention and transformer](https://paperswithcode.com/paper/masked-attention-mask-transformer-for)
  - [Semantic Segmentation](https://paperswithcode.com/task/semantic-segmentation)
  - [Question Answering](https://paperswithcode.com/task/question-answering)
  - [Distillation framework](https://paperswithcode.com/paper/a-fast-knowledge-distillation-framework-for)
- Search papers for the words you are interested in through [http://www.arxiv-sanity.com/](http://www.arxiv-sanity.com/)
- Follow links in communities related to Machine Learning
- Take one of the papers that helped you in some of the competitions

Finally, some sample papers:

- [A Neural Algorithm of Artistic Style](https://arxiv.org/abs/1508.06576) - how did the transfer of style a la Prisma begin
- [Super-convergence](https://arxiv.org/abs/1708.07120) - very fast network training approach
- [SquuezeNet](https://arxiv.org/abs/1602.07360) - mobile optimized architecture example
- [Mask RCNN](https://arxiv.org/abs/1703.06870) - development of RCNN for the Instance Segmentation task
- [DenseNet](https://arxiv.org/abs/1608.06993) - development of the basic CNN architecture after ResNet
- [Learning from Simulated and Unsupervised Images through Adversarial Training](https://arxiv.org/abs/1612.07828) - using GANs to generate examples for training
- [OpenAI GPT-3](https://openai.com/blog/gpt-3-apps/) is one of the latest architectures for learning language models
- [Transformer-XL](https://arxiv.org/abs/1901.02860) - development of the Transformer architecture
- [TacoTron2](https://arxiv.org/abs/1712.05884) - architecture for speech synthesis

## What does it mean «to describe»?

Write a post with a brief description of the problem that the paper is solving, the key idea and result, and post it somewhere! Blog posts on any site, posts in telegram chats, Facebook, Medium, youtube videos, etc., are allowed.

Post Wishes:

- Describe the problem solved in the paper and the main metric. It would be great to give examples of samples in the dataset, say how big it is, etc.
- Describe the key idea of the paper "on the fingers", ideally in the context of past approaches or comparison with a certain baseline. Usually in papers there is a diagram describing the key points, bring it and explain what is on it
- Describe the results. Often at the end of the paper there is a table comparing the approach from the paper with other methods, it should also be given and explained
- Mention that the post is written for https://github.com/avalur/ml-course-kbtu

Here are some similar type of posts in ML courses:
- https://habr.com/en/post/301084/ (in Russian)
- https://medium.com/@acforvs/graph-attention-networks-paper-explained-c8e3039e2f55

## OK, and then what?
Create presentation about your work for final project and add link to your post to the presentation.

We will post the best of them on KBTU social networks and, of course, we will arrange a competition for the number of likes! :)
